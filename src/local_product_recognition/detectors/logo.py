"""Brand logo detection via OCR text matching with template fallback."""

from __future__ import annotations

from dataclasses import dataclass
from importlib import resources
from typing import List, Optional, Sequence, Tuple

import cv2
import numpy as np

from ..image_utils import ensure_gray
from ..ocr import LightweightTextRecognizer
from ..types import DetectionResult, Feature
from .base import FeatureDetector

BrandKeyword = Tuple[str, str]  # (normalized_value, display_label)


@dataclass(frozen=True)
class _LogoTemplate:
    name: str
    brand: str
    image: np.ndarray


class BrandLogoDetector(FeatureDetector):
    """Detects brand logos either through OCR text matching or template search."""

    def __init__(
        self,
        similarity_threshold: float = 0.55,
        brand_keywords: Optional[Sequence[str]] = None,
        text_recognizer: Optional[LightweightTextRecognizer] = None,
    ) -> None:
        super().__init__(Feature.BRAND_LOGO)
        self.similarity_threshold = similarity_threshold
        self._templates = self._load_templates()
        self.brand_keywords: List[BrandKeyword] = self._prepare_brand_keywords(
            brand_keywords, self._templates
        )
        self._ocr = text_recognizer or LightweightTextRecognizer()

    def detect(self, image: np.ndarray) -> Optional[DetectionResult]:
        ocr_detection = self._detect_via_text(image)
        if ocr_detection is not None:
            return ocr_detection
        
        # å³ä½¿æ²¡æœ‰åŒ¹é…å“ç‰Œï¼Œä¹Ÿå°è¯•è¿”å›ž OCR è¯†åˆ«çš„æ–‡æœ¬ï¼ˆä½Žç½®ä¿¡åº¦ï¼‰
        unmatched_ocr = self._get_all_ocr_texts(image)
        if unmatched_ocr:
            return unmatched_ocr
        
        return self._detect_via_templates(image)

    # ------------------------------------------------------------------
    # OCR-based detection
    # ------------------------------------------------------------------

    def _detect_via_text(self, image: np.ndarray) -> Optional[DetectionResult]:
        if not self.brand_keywords:
            return None

        try:
            text_regions = self._ocr.detect(image)
        except Exception as e:
            # OCR æ£€æµ‹å¤±è´¥ï¼Œé™é»˜è·³è¿‡
            return None
        
        if not text_regions:
            return None

        # è®°å½•æ‰€æœ‰ OCR è¯†åˆ«çš„æ–‡æœ¬ï¼ˆç”¨äºŽè°ƒè¯•ï¼‰
        all_recognized_texts = [region.text for region in text_regions]
        if all_recognized_texts:
            print(f"    ðŸ” OCR è¯†åˆ«åˆ° {len(all_recognized_texts)} ä¸ªæ–‡æœ¬åŒºåŸŸ: {', '.join(all_recognized_texts[:3])}{'...' if len(all_recognized_texts) > 3 else ''}")
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå“ç‰Œå…³é”®å­—ï¼ˆä»…å‰3ä¸ªï¼‰
        if self.brand_keywords:
            print(f"    ðŸ”‘ åŒ¹é…å…³é”®å­—: {', '.join([kw[1] for kw in self.brand_keywords[:3]])}...")

        for region in text_regions:
            normalized_text = self._normalize_label(region.text)
            if not normalized_text:
                continue
            
            # å°è¯•åŒ¹é…å“ç‰Œå…³é”®å­—
            for normalized_keyword, label in self.brand_keywords:
                if normalized_keyword and normalized_keyword in normalized_text:
                    x, y, w, h = region.bounding_box
                    print(f"    âœ… OCR åŒ¹é…æˆåŠŸ: '{region.text}' åŒ¹é…å“ç‰Œ '{label}'")
                    return DetectionResult(
                        feature=self.feature,
                        confidence=self._clip_confidence(region.confidence),
                        details={
                            "method": "ocr",
                            "brand": label,
                            "recognized_text": region.text,
                            "bounding_box": [int(x), int(y), int(w), int(h)],
                        },
                    )
                # åå‘åŒ¹é…ï¼šæ£€æŸ¥å“ç‰Œå…³é”®å­—æ˜¯å¦åœ¨è¯†åˆ«æ–‡æœ¬ä¸­
                # è¦æ±‚ï¼šè¯†åˆ«æ–‡æœ¬è‡³å°‘3ä¸ªå­—ç¬¦ï¼Œä¸”åŒ¹é…é•¿åº¦ >= 4
                elif (normalized_keyword and 
                      len(normalized_text) >= 3 and 
                      len(normalized_keyword) >= 4 and 
                      normalized_text in normalized_keyword):
                    x, y, w, h = region.bounding_box
                    print(f"    âœ… OCR éƒ¨åˆ†åŒ¹é…: '{region.text}' éƒ¨åˆ†åŒ¹é…å“ç‰Œ '{label}'")
                    return DetectionResult(
                        feature=self.feature,
                        confidence=self._clip_confidence(region.confidence * 0.8),  # é™ä½Žç½®ä¿¡åº¦
                        details={
                            "method": "ocr",
                            "brand": label,
                            "recognized_text": region.text,
                            "bounding_box": [int(x), int(y), int(w), int(h)],
                            "match_type": "partial"
                        },
                    )
        
        # OCR è¯†åˆ«äº†æ–‡æœ¬ï¼Œä½†æ²¡æœ‰åŒ¹é…å“ç‰Œ
        if all_recognized_texts:
            print(f"    âš ï¸  OCR è¯†åˆ«äº†æ–‡æœ¬ï¼Œä½†æœªåŒ¹é…ä»»ä½•å“ç‰Œå…³é”®å­—")
        return None
    
    def _get_all_ocr_texts(self, image: np.ndarray) -> Optional[DetectionResult]:
        """èŽ·å–æ‰€æœ‰ OCR è¯†åˆ«çš„æ–‡æœ¬ï¼Œå³ä½¿æ²¡æœ‰åŒ¹é…å“ç‰Œä¹Ÿè¿”å›ž"""
        try:
            text_regions = self._ocr.detect(image)
        except Exception:
            return None
        
        if not text_regions:
            return None
        
        # åˆå¹¶æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
        all_texts = [region.text for region in text_regions if region.text.strip()]
        if not all_texts:
            return None
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(region.confidence for region in text_regions) / len(text_regions)
        
        # è¿”å›žä½Žç½®ä¿¡åº¦çš„ç»“æžœï¼ŒåŒ…å«æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
        print(f"    ðŸ“ ä¿å­˜æœªåŒ¹é…çš„ OCR æ–‡æœ¬: {', '.join(all_texts[:3])}{'...' if len(all_texts) > 3 else ''}")
        return DetectionResult(
            feature=self.feature,
            confidence=self._clip_confidence(avg_confidence * 0.3),  # ä½Žç½®ä¿¡åº¦
            details={
                "method": "ocr",
                "brand": None,
                "recognized_texts": all_texts,  # æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
                "match_type": "unmatched",
                "text_count": len(all_texts)
            },
        )

    # ------------------------------------------------------------------
    # Template fallback
    # ------------------------------------------------------------------

    def _detect_via_templates(self, image: np.ndarray) -> Optional[DetectionResult]:
        if not self._templates:
            return None

        gray = ensure_gray(image)
        gray = cv2.GaussianBlur(gray, (3, 3), 0)
        best_match: Optional[Tuple[_LogoTemplate, float, Tuple[int, int]]] = None

        for template in self._templates:
            th, tw = template.image.shape[:2]
            if gray.shape[0] < th or gray.shape[1] < tw:
                continue

            result = cv2.matchTemplate(gray, template.image, cv2.TM_CCOEFF_NORMED)
            _, max_val, _, max_loc = cv2.minMaxLoc(result)
            if max_val < self.similarity_threshold:
                continue
            if best_match is None or max_val > best_match[1]:
                best_match = (template, float(max_val), max_loc)

        if best_match is None:
            return None

        template, score, location = best_match
        return DetectionResult(
            feature=self.feature,
            confidence=self._clip_confidence(score),
            details={
                "method": "template",
                "template": template.name,
                "brand": template.brand,
                "top_left": [int(location[0]), int(location[1])],
            },
        )

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------

    def _load_templates(self) -> List[_LogoTemplate]:
        templates: List[_LogoTemplate] = []
        try:
            data_root = resources.files("local_product_recognition").joinpath("data", "logos")
        except FileNotFoundError:
            return templates

        for entry in data_root.iterdir():
            if entry.suffix.lower() not in {".png", ".jpg", ".jpeg"}:
                continue
            raw = np.frombuffer(entry.read_bytes(), dtype=np.uint8)
            image = cv2.imdecode(raw, cv2.IMREAD_GRAYSCALE)
            if image is None:
                continue
            templates.append(
                _LogoTemplate(name=entry.name, brand=entry.stem.upper(), image=image)
            )
        return templates

    def _prepare_brand_keywords(
        self,
        provided: Optional[Sequence[str]],
        templates: Sequence[_LogoTemplate],
    ) -> List[BrandKeyword]:
        raw_keywords: List[str] = []
        if provided:
            raw_keywords.extend(provided)
        raw_keywords.extend(template.brand for template in templates)

        keywords: List[BrandKeyword] = []
        seen: set[str] = set()
        for keyword in raw_keywords:
            label = keyword.strip().upper()
            normalized = self._normalize_label(label)
            if not label or not normalized or normalized in seen:
                continue
            seen.add(normalized)
            keywords.append((normalized, label))
        keywords.sort(key=lambda item: (-len(item[0]), item[0]))
        return keywords

    @staticmethod
    def _normalize_label(value: str) -> str:
        return "".join(ch for ch in value.upper() if ch.isalnum())
